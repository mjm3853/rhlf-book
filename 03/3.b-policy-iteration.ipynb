{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming\n",
    "\n",
    "We will be using dynamic programming on sample 4x4 grid world and study **Policy Iteration** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration (Improvement)\n",
    "\n",
    "Policy improvement is carried out by repeatedly applying policy evaluation and greedy action selection steps in a cycle till there is no further change. Pseudo code for the algorithm is given in Fig 3-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "### Policy Iteration (Improvement)\n",
    "\n",
    "Policy iteration is a method used in reinforcement learning to find the best way for an agent to act in an environment to achieve its goals. It involves two main steps: policy evaluation and policy improvement.\n",
    "\n",
    "1. **Policy Evaluation**: This step calculates how good the current way of acting (policy) is by estimating the expected rewards the agent will get by following this policy.\n",
    "\n",
    "2. **Policy Improvement**: This step updates the policy to make it better by choosing actions that lead to higher rewards based on the evaluation.\n",
    "\n",
    "These steps are repeated in a cycle until the policy stops changing, meaning the agent has found the best way to act. This process ensures that the agent learns the optimal actions to take in different situations to maximize its rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running in Colab/Kaggle\n",
    "\n",
    "If you are running this on Colab, please uncomment below cell and run this to install required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment and execute this cell to install all the the dependencies if running in Google Colab or Kaggle\n",
    "\n",
    "## Uncomment and run for Colab\n",
    "# !git clone https://github.com/nsanghi/drl-2ed\n",
    "# %cd /content/drl-2ed \n",
    "# %cd chapter3\n",
    "\n",
    "\n",
    "## Uncomment and run for Kaggle\n",
    "# !git clone https://github.com/nsanghi/drl-2ed\n",
    "# %cd /content/drl-2ed \n",
    "# %cd chapter3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and environment setup\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# create grid world environment\n",
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom print to show state values inside the grid\n",
    "def grid_print(V, k=None):\n",
    "    ax = sns.heatmap(V.reshape(env.shape),\n",
    "                     annot=True, square=True,\n",
    "                     cbar=False, cmap='Blues',\n",
    "                     xticklabels=False, yticklabels=False)\n",
    "\n",
    "    if k:\n",
    "        ax.set(title=\"State values after K = {0}\".format(k))\n",
    "    else:\n",
    "        ax.set(title=\"State Values\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "def policy_evaluation(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and\n",
    "    a full description of the environment's dynamics.\n",
    "\n",
    "    Args:\n",
    "        policy:[S, A] shaped matrix representing the policy. Random in our case\n",
    "        env: env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        theta: Stop evaluation once value function change is\n",
    "            less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    V_new = np.copy(V)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # For each state, perform a \"backup\"\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, pi_a in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value as per backup diagram\n",
    "                    v += pi_a * prob * \\\n",
    "                        (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            V_new[s] = v\n",
    "            delta = max(delta, np.abs(V_new[s] - V[s]))\n",
    "        V = np.copy(V_new)\n",
    "        # Stop if change is below a threshold\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Improvement\n",
    "\n",
    "def policy_improvement(policy, V, env, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Improve a policy given an environment and a full description\n",
    "    of the environment's dynamics and the state-values V.\n",
    "\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        V: current state-value for the given policy\n",
    "        env: env.P -> transition dynamics of the environment.\n",
    "            env.P[s][a] [(prob, next_state, reward, done)].\n",
    "            env.nS is number of states in the environment.\n",
    "            env.nA is number of actions in the environment.\n",
    "        discount_factor: Gamma discount factor.\n",
    "\n",
    "    Returns:\n",
    "        policy: [S, A] shaped matrix representing improved policy.\n",
    "        policy_changed: boolean which has value of `True` if there\n",
    "                        was a change in policy\n",
    "    \"\"\"\n",
    "\n",
    "    def argmax_a(arr):\n",
    "        \"\"\"\n",
    "        Return idxs of all max values in an array.\n",
    "        \"\"\"\n",
    "        max_idx = []\n",
    "        max_val = float('-inf')\n",
    "        for idx, elem in enumerate(arr):\n",
    "            if elem == max_val:\n",
    "                max_idx.append(idx)\n",
    "            elif elem > max_val:\n",
    "                max_idx = [idx]\n",
    "                max_val = elem\n",
    "        return max_idx\n",
    "\n",
    "    policy_changed = False\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    new_policy = np.zeros([env.nS, env.nA])\n",
    "\n",
    "    # For each state, perform a \"greedy improvement\"\n",
    "    for s in range(env.nS):\n",
    "        old_action = np.array(policy[s])\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[s][a]:\n",
    "                # Calculate the expected value as per backup diagram\n",
    "                Q[s, a] += prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "        # get maximizing actions and set new policy for state s\n",
    "        best_actions = argmax_a(Q[s])\n",
    "        new_policy[s, best_actions] = 1.0 / len(best_actions)\n",
    "\n",
    "    if not np.allclose(new_policy[s], policy[s]):\n",
    "        policy_changed = True\n",
    "\n",
    "    return new_policy, policy_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration\n",
    "\n",
    "def policy_iteration(env, discount_factor=1.0, theta=0.00001):\n",
    "\n",
    "    # initialize a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, env, discount_factor, theta)\n",
    "        policy, changed = policy_improvement(policy, V, env, discount_factor)\n",
    "        if not changed:  # terminate iteration once no improvement is observed\n",
    "            V_optimal = policy_evaluation(policy, env, discount_factor, theta)\n",
    "            print(\"Optimal Policy\\n\", policy)\n",
    "            return np.array(V_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy\n",
      " [[0.25 0.25 0.25 0.25]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.   1.  ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.   0.   0.5 ]\n",
      " [0.   0.   0.5  0.5 ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.5  0.5  0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   0.   1.   0.  ]\n",
      " [1.   0.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.   1.   0.   0.  ]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run policy iteration on Grid world\n",
    "V_star = policy_iteration(env)\n",
    "\n",
    "# Print optimal policy state values\n",
    "grid_print(V_star.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We see that optimal state values are negative of number of steps required to reach the closest terminal state. As the reward is -1 for each time step till agent reaches terminal state, the optimal policy would take the agent to terminal state in minimal number of possible steps. For some states, more than one action could lead to same number of steps to reach terminal state. For example, look at top right state with value -3, it takes 3 steps to reach the terminal state at top-left or terminal state at bottom-right. In other words, the state values is negative of Manhattan distance between the state and nearest terminal state."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
