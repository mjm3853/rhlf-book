{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Ch 03\n",
    "\n",
    "## Prompt To Kick Things Off\n",
    "\n",
    "```\n",
    "As an expert in Deep Reinforcement Learning with Python, help teach concepts to an ML Engineer who does more engineering than data science. To start, write some brief notes about Deep Reinforcement Learning\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Brief Notes on Deep Reinforcement Learning\n",
    "\n",
    "Deep Reinforcement Learning (DRL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. Here are some key concepts:\n",
    "\n",
    "1. **Reinforcement Learning (RL)**: RL is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize cumulative reward. The agent receives feedback in the form of rewards or penalties and uses this feedback to improve its future actions.\n",
    "\n",
    "2. **Deep Learning**: Deep learning involves neural networks with many layers (deep neural networks) that can learn complex patterns in data. In the context of DRL, deep learning is used to approximate the value functions or policies that the RL agent uses to make decisions.\n",
    "\n",
    "3. **Agent**: The entity that interacts with the environment and learns to make decisions.\n",
    "\n",
    "4. **Environment**: The external system with which the agent interacts. The environment provides feedback to the agent in the form of rewards or penalties.\n",
    "\n",
    "5. **State**: A representation of the current situation of the environment. The state is used by the agent to decide the next action.\n",
    "\n",
    "6. **Action**: The set of all possible moves the agent can make at any given state.\n",
    "\n",
    "7. **Reward**: The feedback from the environment in response to the agent's action. The goal of the agent is to maximize the cumulative reward over time.\n",
    "\n",
    "8. **Policy**: A strategy used by the agent to determine the next action based on the current state. In DRL, policies can be represented by deep neural networks.\n",
    "\n",
    "9. **Value Function**: A function that estimates the expected cumulative reward of a state or state-action pair. Value functions can also be approximated using deep neural networks in DRL.\n",
    "\n",
    "10. **Exploration vs. Exploitation**: The dilemma of choosing between exploring new actions to discover their effects and exploiting known actions that yield high rewards.\n",
    "\n",
    "11. **Popular Algorithms**: Some popular DRL algorithms include Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Actor-Critic methods.\n",
    "\n",
    "DRL has been successfully applied to various domains, including robotics, game playing (e.g., AlphaGo), and autonomous driving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Examples of Rewards in Deep Reinforcement Learning\n",
    "\n",
    "In Deep Reinforcement Learning (DRL), rewards are crucial as they guide the learning process of the agent. Here are some examples of rewards and an explanation of how the model receives and utilizes them:\n",
    "\n",
    "### Examples of Rewards\n",
    "\n",
    "1. **Game Playing**: In a game like chess or Go, the reward can be +1 for a win, -1 for a loss, and 0 for a draw. Intermediate rewards can also be given for capturing pieces or achieving certain positions.\n",
    "\n",
    "2. **Robotics**: In a robotic arm task, the reward can be based on the distance to a target object. For example, a small negative reward for each time step the arm is not at the target, and a large positive reward when the arm successfully grasps the object.\n",
    "\n",
    "3. **Autonomous Driving**: In autonomous driving, the reward can be based on safety and efficiency. For example, a positive reward for staying in the lane and reaching the destination, and a negative reward for collisions or traffic violations.\n",
    "\n",
    "4. **Resource Management**: In a data center, the reward can be based on energy efficiency. For example, a positive reward for reducing energy consumption while maintaining performance, and a negative reward for excessive energy use.\n",
    "\n",
    "### How the Model Receives and Utilizes the Reward\n",
    "\n",
    "1. **Receiving the Reward**: The agent interacts with the environment by taking actions. After each action, the environment provides feedback in the form of a reward. This reward indicates how good or bad the action was in achieving the agent's goal.\n",
    "\n",
    "2. **Utilizing the Reward**: The agent uses the received rewards to update its policy or value function. The goal is to maximize the cumulative reward over time. This is typically done using algorithms like Q-learning or policy gradients, which adjust the agent's parameters to improve its decision-making process.\n",
    "\n",
    "3. **Temporal Difference Learning**: In many DRL algorithms, the agent uses temporal difference learning to update its value estimates. This involves comparing the predicted value of a state-action pair with the actual reward received plus the estimated value of the next state.\n",
    "\n",
    "4. **Exploration vs. Exploitation**: The agent must balance exploration (trying new actions to discover their rewards) and exploitation (choosing actions that are known to yield high rewards). This balance is crucial for effective learning.\n",
    "\n",
    "By continuously interacting with the environment and receiving rewards, the agent learns to make better decisions that maximize its cumulative reward over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "## Transition Dynamics in Deep Reinforcement Learning\n",
    "\n",
    "Transition dynamics are a fundamental concept in Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL). They describe how the environment changes in response to the agent's actions. Understanding transition dynamics is crucial for designing effective RL algorithms. Here are the key points:\n",
    "\n",
    "1. **Definition**: Transition dynamics refer to the probability distribution of the next state given the current state and action. Formally, it is represented as \\( P(s' | s, a) \\), where \\( s \\) is the current state, \\( a \\) is the action taken by the agent, and \\( s' \\) is the next state.\n",
    "\n",
    "2. **Markov Decision Process (MDP)**: Transition dynamics are a core component of the Markov Decision Process (MDP), which is the mathematical framework used to model RL problems. An MDP is defined by:\n",
    "    - A set of states \\( S \\)\n",
    "    - A set of actions \\( A \\)\n",
    "    - A transition function \\( P(s' | s, a) \\)\n",
    "    - A reward function \\( R(s, a) \\)\n",
    "    - A discount factor \\( \\gamma \\)\n",
    "\n",
    "3. **Deterministic vs. Stochastic**: Transition dynamics can be deterministic or stochastic.\n",
    "    - **Deterministic**: The next state is uniquely determined by the current state and action. For example, in a simple grid world, moving right from a specific cell always leads to the same next cell.\n",
    "    - **Stochastic**: The next state is determined probabilistically. For example, in a game with dice rolls, the outcome of an action depends on the roll of the dice.\n",
    "\n",
    "4. **Model-Based vs. Model-Free RL**:\n",
    "    - **Model-Based RL**: The agent explicitly learns or is given the transition dynamics \\( P(s' | s, a) \\) and uses this model to plan its actions.\n",
    "    - **Model-Free RL**: The agent does not learn the transition dynamics explicitly. Instead, it learns a policy or value function directly from interactions with the environment.\n",
    "\n",
    "5. **Importance in Learning**: Understanding transition dynamics helps the agent predict the consequences of its actions, which is essential for planning and decision-making. In model-based RL, accurate transition dynamics enable the agent to simulate future states and rewards, leading to more informed actions.\n",
    "\n",
    "6. **Challenges**: Learning accurate transition dynamics can be challenging, especially in complex environments with high-dimensional state and action spaces. Approximation methods, such as neural networks, are often used to model transition dynamics in DRL.\n",
    "\n",
    "In summary, transition dynamics describe how the environment responds to the agent's actions and are a key component of the MDP framework in RL. They play a crucial role in the agent's ability to learn and make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . .\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: \n",
      "\n",
      "* A . .\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right\n",
      "\n",
      "* A . .\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right -> down\n",
      "\n",
      "* A . .\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right -> down -> down\n",
      "\n",
      "* * A .\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right\n",
      "\n",
      "* * * A\n",
      ". O . .\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right -> right\n",
      "\n",
      "* * * *\n",
      ". O . A\n",
      ". . O .\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right -> right -> down\n",
      "\n",
      "* * * *\n",
      ". O . *\n",
      ". . O A\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right -> right -> down -> down\n",
      "\n",
      "* * * *\n",
      ". O . *\n",
      ". . O A\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right -> right -> down -> down -> right\n",
      "\n",
      "* * * *\n",
      ". O . *\n",
      ". . O A\n",
      ". . . T\n",
      "Actions taken: right -> down -> down -> right -> right -> down -> down -> right -> right\n",
      "\n",
      "* * * *\n",
      ". O . *\n",
      ". . O *\n",
      ". . . A\n",
      "Actions taken: right -> down -> down -> right -> right -> down -> down -> right -> right -> down\n",
      "\n",
      "Reached terminal state!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4, obstacles=None, random_rewards=False):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size))  # Initialize the grid with zeros\n",
    "        self.start_state = (0, 0)  # Define the start state\n",
    "        self.terminal_states = [(size-1, size-1)]  # Define terminal states\n",
    "        self.current_state = self.start_state  # Set the current state to the start state\n",
    "        self.previous_states = []  # Keep track of previous states\n",
    "        self.actions_taken = []  # Keep track of actions taken\n",
    "        self.obstacles = obstacles if obstacles else []  # Define obstacles\n",
    "        self.random_rewards = random_rewards  # Flag for random rewards\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = self.start_state  # Reset the current state to the start state\n",
    "        self.previous_states = []  # Reset the previous states\n",
    "        self.actions_taken = []  # Reset the actions taken\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # If the current state is a terminal state, return it with reward 0 and done True\n",
    "        if self.current_state in self.terminal_states:\n",
    "            return self.current_state, 0, True\n",
    "\n",
    "        x, y = self.current_state\n",
    "        # Update the state based on the action\n",
    "        if action == 'up':\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 'down':\n",
    "            x = min(self.size - 1, x + 1)\n",
    "        elif action == 'left':\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 'right':\n",
    "            y = min(self.size - 1, y + 1)\n",
    "\n",
    "        new_state = (x, y)\n",
    "        # Check for obstacles\n",
    "        if new_state in self.obstacles:\n",
    "            new_state = self.current_state  # Stay in the same state if there's an obstacle\n",
    "            reward = -10  # Negative reward for hitting an obstacle\n",
    "        else:\n",
    "            # Reward based on distance to the terminal state\n",
    "            old_distance = np.sum(np.abs(np.array(self.current_state) - np.array(self.terminal_states[0])))\n",
    "            new_distance = np.sum(np.abs(np.array(new_state) - np.array(self.terminal_states[0])))\n",
    "            reward = 1 if new_distance < old_distance else -1\n",
    "\n",
    "        self.previous_states.append(self.current_state)  # Add current state to previous states\n",
    "        self.actions_taken.append(action)  # Add action to actions taken\n",
    "        self.current_state = new_state  # Update the current state\n",
    "\n",
    "        done = self.current_state in self.terminal_states  # Check if the current state is terminal\n",
    "        return self.current_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.zeros((self.size, self.size), dtype=str)  # Create an empty grid\n",
    "        grid[:] = '.'  # Fill the grid with dots\n",
    "        for state in self.terminal_states:\n",
    "            grid[state] = 'T'  # Mark terminal states with 'T'\n",
    "        for state in self.obstacles:\n",
    "            grid[state] = 'O'  # Mark obstacles with 'O'\n",
    "        for state in self.previous_states:\n",
    "            grid[state] = '*'  # Mark previous states with '*'\n",
    "        x, y = self.current_state\n",
    "        grid[x, y] = 'A'  # Mark the agent's current position with 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid))  # Print the grid\n",
    "        print(\"Actions taken:\", ' -> '.join(self.actions_taken))  # Print the actions taken\n",
    "        print()\n",
    "\n",
    "# Create and test the grid world environment with obstacles and random rewards\n",
    "obstacles = [(1, 1), (2, 2)]\n",
    "env = GridWorld(size=4, obstacles=obstacles, random_rewards=True)\n",
    "env.render()\n",
    "\n",
    "# Automatically take actions until the terminal state is reached\n",
    "## Might get to infinite state ... actions = ['up', 'down', 'left', 'right']\n",
    "done = False\n",
    "while not done:\n",
    "    for action in actions:\n",
    "        state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Reached terminal state!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
